{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This process may take 5 to 10 minutes, depending on the performance of your computer.\n",
      "\n",
      "\n",
      "\n",
      "The pre-processing phase of the  Monday-WorkingHours.pcap_ISCX  file is completed.\n",
      "\n",
      "The pre-processing phase of the  Tuesday-WorkingHours.pcap_ISCX  file is completed.\n",
      "\n",
      "The pre-processing phase of the  Wednesday-workingHours.pcap_ISCX  file is completed.\n",
      "\n",
      "The pre-processing phase of the  Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX  file is completed.\n",
      "\n",
      "The pre-processing phase of the  Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX  file is completed.\n",
      "\n",
      "The pre-processing phase of the  Friday-WorkingHours-Morning.pcap_ISCX  file is completed.\n",
      "\n",
      "The pre-processing phase of the  Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX  file is completed.\n",
      "\n",
      "The pre-processing phase of the  Friday-WorkingHours-Afternoon-DDos.pcap_ISCX  file is completed.\n",
      "\n",
      "mission accomplished!\n",
      "Total operation time: =  79.64361929893494 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "seconds = time.time()\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"This process may take 5 to 10 minutes, depending on the performance of your computer.\\n\\n\\n\")\n",
    "number=\"0123456789\"\n",
    "# CSV files names:\n",
    "path = \"/teamspace/studios/this_studio/Anomoly_Threat_Detection_using_GNNs/datasets/cicids2017/\"\n",
    "csv_files=[\"Monday-WorkingHours.pcap_ISCX\",\n",
    "        \"Tuesday-WorkingHours.pcap_ISCX\",\n",
    "        \"Wednesday-workingHours.pcap_ISCX\",\n",
    "        \"Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX\",\n",
    "        \"Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX\",\n",
    "        \"Friday-WorkingHours-Morning.pcap_ISCX\",\n",
    "        \"Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX\",\n",
    "        \"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX\",]\n",
    "\n",
    "# Headers of column\n",
    "main_labels=[\"Flow ID\",\"Source IP\",\"Source Port\",\"Destination IP\",\"Destination Port\",\"Protocol\",\"Timestamp\",\"Flow Duration\",\"Total Fwd Packets\",\n",
    "   \"Total Backward Packets\",\"Total Length of Fwd Packets\",\"Total Length of Bwd Packets\",\"Fwd Packet Length Max\",\"Fwd Packet Length Min\",\n",
    "   \"Fwd Packet Length Mean\",\"Fwd Packet Length Std\",\"Bwd Packet Length Max\",\"Bwd Packet Length Min\",\"Bwd Packet Length Mean\",\"Bwd Packet Length Std\",\n",
    "   \"Flow Bytes/s\",\"Flow Packets/s\",\"Flow IAT Mean\",\"Flow IAT Std\",\"Flow IAT Max\",\"Flow IAT Min\",\"Fwd IAT Total\",\"Fwd IAT Mean\",\"Fwd IAT Std\",\"Fwd IAT Max\",\n",
    "   \"Fwd IAT Min\",\"Bwd IAT Total\",\"Bwd IAT Mean\",\"Bwd IAT Std\",\"Bwd IAT Max\",\"Bwd IAT Min\",\"Fwd PSH Flags\",\"Bwd PSH Flags\",\"Fwd URG Flags\",\"Bwd URG Flags\",\n",
    "   \"Fwd Header Length\",\"Bwd Header Length\",\"Fwd Packets/s\",\"Bwd Packets/s\",\"Min Packet Length\",\"Max Packet Length\",\"Packet Length Mean\",\"Packet Length Std\",\n",
    "   \"Packet Length Variance\",\"FIN Flag Count\",\"SYN Flag Count\",\"RST Flag Count\",\"PSH Flag Count\",\"ACK Flag Count\",\"URG Flag Count\",\"CWE Flag Count\",\n",
    "   \"ECE Flag Count\",\"Down/Up Ratio\",\"Average Packet Size\",\"Avg Fwd Segment Size\",\"Avg Bwd Segment Size\",\"faulty-Fwd Header Length\",\"Fwd Avg Bytes/Bulk\",\n",
    "   \"Fwd Avg Packets/Bulk\",\"Fwd Avg Bulk Rate\",\"Bwd Avg Bytes/Bulk\",\"Bwd Avg Packets/Bulk\",\"Bwd Avg Bulk Rate\",\"Subflow Fwd Packets\",\"Subflow Fwd Bytes\",\n",
    "   \"Subflow Bwd Packets\",\"Subflow Bwd Bytes\",\"Init_Win_bytes_forward\",\"Init_Win_bytes_backward\",\"act_data_pkt_fwd\",\n",
    "   \"min_seg_size_forward\",\"Active Mean\",\"Active Std\",\"Active Max\",\"Active Min\",\"Idle Mean\",\"Idle Std\",\"Idle Max\",\"Idle Min\",\"Label\",\"External IP\"]\n",
    "\n",
    "main_labels2=main_labels\n",
    "main_labels=( \",\".join( i for i in main_labels ) )\n",
    "main_labels=main_labels+\"\\n\"\n",
    "flag=True\n",
    "for i in range(len(csv_files)):\n",
    "    ths = open(str(i)+\".csv\", \"w\")\n",
    "    ths.write(main_labels)\n",
    "    with open(f\"{path}\"+csv_files[i]+\".csv\", \"r\") as file:\n",
    "        while True:\n",
    "            try:\n",
    "                line=file.readline()\n",
    "                if  line[0] in number:# this line eliminates the headers of CSV files and incomplete streams .\n",
    "                    if \" – \" in str(line): ##  if there is \"–\" character (\"–\", Unicode code:8211) in the flow ,  it will be chanced with \"-\" character ( Unicode code:45).\n",
    "                        line=(str(line).replace(\" – \",\" - \"))\n",
    "                    line=(str(line).replace(\"inf\",\"0\"))\n",
    "                    line=(str(line).replace(\"Infinity\",\"0\"))\n",
    "                    line=(str(line).replace(\"NaN\",\"0\"))\n",
    "                     \n",
    "                    ths.write(str(line))\n",
    "                else:\n",
    "                    continue                       \n",
    "            except:\n",
    "                break\n",
    "    ths.close()\n",
    " \n",
    " \n",
    "    df=pd.read_csv(str(i)+\".csv\",low_memory=False)\n",
    "    df=df.fillna(0)\n",
    "\n",
    "    string_features=[\"Flow Bytes/s\",\"Flow Packets/s\"]\n",
    "    for ii in string_features: #Some data in the \"Flow Bytes / s\" and \"Flow Packets / s\" columns are not numeric. Fixing this bug in this loop\n",
    "        df[ii]=df[ii].replace('Infinity', -1)\n",
    "        df[ii]=df[ii].replace('NaN', 0)\n",
    "        number_or_not=[]\n",
    "        for iii in df[ii]:\n",
    "            try:\n",
    "                k=int(float(iii))\n",
    "                number_or_not.append(int(k))\n",
    "            except:\n",
    "                number_or_not.append(iii)\n",
    "        df[ii]=number_or_not\n",
    "\n",
    "\n",
    "\n",
    "    string_features=[]\n",
    "    for j in main_labels2: # In this section, non-numeric (string and / or categorical) properties (columns) are detected.\n",
    "        if df[j].dtype==\"object\":\n",
    "            string_features.append(j)\n",
    "    try:\n",
    "        string_features.remove('Label')#The \"Label\" property was removed from the list. Because it has to remain \"categorical\" for using with different machine learning approach.\n",
    "    except:\n",
    "        print(\"error!\")\n",
    "    labelencoder_X = preprocessing.LabelEncoder()\n",
    "\n",
    "\n",
    "\n",
    "    for ii in string_features: ## In this loop, non-numeric (string and/or categorical) properties converted to numeric features.\n",
    "        try:\n",
    "            df[ii]=labelencoder_X.fit_transform(df[ii])\n",
    "        except:\n",
    "            df[ii]=df[ii].replace('Infinity', -1)\n",
    "    df=df.drop(main_labels2[61], axis=1) ## Column 61 is deleted because it is unnecessary, column 41 (\"Fwd Header Length\" feature) had be mistakenly rewritten.\n",
    "\n",
    "\n",
    "\n",
    "    ##All CSV files are merged into a single file.\n",
    "    if flag:\n",
    "        df.to_csv('/teamspace/studios/this_studio/Anomoly_Threat_Detection_using_GNNs/datasets/cicids2017/all_data.csv' ,index = False)\n",
    "        flag=False\n",
    "    else:\n",
    "        df.to_csv('/teamspace/studios/this_studio/Anomoly_Threat_Detection_using_GNNs/datasets/cicids2017/all_data.csv' ,index = False,header=False,mode=\"a\")\n",
    "    os.remove(str(i)+\".csv\")\n",
    "    print(\"The pre-processing phase of the \",csv_files[i],\" file is completed.\\n\")\n",
    "    \n",
    "\n",
    "print(\"mission accomplished!\")\n",
    "print(\"Total operation time: = \",time.time()- seconds ,\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/teamspace/studios/this_studio/Anomoly_Threat_Detection_using_GNNs/datasets/cicids2017/all_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/teamspace/studios/this_studio/Anomoly_Threat_Detection_using_GNNs/datasets/cicids2017/all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1848/186756063.py:68: DtypeWarning: Columns (84) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('/teamspace/studios/this_studio/Anomoly_Threat_Detection_using_GNNs/datasets/cicids2017/all_data.csv', dtype= dtype_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Label Distribution:\n",
      "Label\n",
      "BENIGN              2203723\n",
      "DoS Hulk             231073\n",
      "PortScan             158930\n",
      "DDoS                  41835\n",
      "DoS GoldenEye         10293\n",
      "FTP-Patator            7938\n",
      "SSH-Patator            5897\n",
      "DoS slowloris          5796\n",
      "DoS Slowhttptest       5499\n",
      "Bot                    1966\n",
      "Infiltration             36\n",
      "Heartbleed               11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sampled Label Distribution:\n",
      "Label\n",
      "BENIGN              25000\n",
      "FTP-Patator          2272\n",
      "SSH-Patator          2272\n",
      "DoS slowloris        2272\n",
      "DoS Slowhttptest     2272\n",
      "DoS Hulk             2272\n",
      "DoS GoldenEye        2272\n",
      "DDoS                 2272\n",
      "PortScan             2272\n",
      "Bot                  1966\n",
      "Infiltration           36\n",
      "Heartbleed             11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sampled dataset saved to: /teamspace/studios/this_studio/Anomoly_Threat_Detection_using_GNNs/datasets/cicids2017/s4_all_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def sample_balanced_dataset(df, total_rows=20000, benign_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Sample a balanced dataset with proportional representation of labels.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        total_rows (int): Total number of rows in the sampled dataset\n",
    "        benign_ratio (float): Proportion of benign samples (default 70%)\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Sampled and balanced dataset\n",
    "    \"\"\"\n",
    "    # Get label distribution\n",
    "    label_counts = df['Label'].value_counts()\n",
    "    print(\"Original Label Distribution:\")\n",
    "    print(label_counts)\n",
    "    \n",
    "    # Separate benign and malicious samples\n",
    "    benign_samples = df[df['Label'] == 'BENIGN']\n",
    "    malicious_samples = df[df['Label'] != 'BENIGN']\n",
    "    \n",
    "    # Calculate row allocation\n",
    "    benign_rows = int(total_rows * benign_ratio)\n",
    "    malicious_rows = total_rows - benign_rows\n",
    "    \n",
    "    # Sample benign rows\n",
    "    sampled_benign = benign_samples.sample(n=min(benign_rows, len(benign_samples)), \n",
    "                                           random_state=42)\n",
    "    \n",
    "    # Get unique malicious labels\n",
    "    malicious_labels = malicious_samples['Label'].unique()\n",
    "    \n",
    "    # Calculate rows per malicious label\n",
    "    malicious_label_rows = malicious_rows // len(malicious_labels)\n",
    "    \n",
    "    # Sample malicious rows\n",
    "    sampled_malicious = pd.DataFrame()\n",
    "    for label in malicious_labels:\n",
    "        label_samples = malicious_samples[malicious_samples['Label'] == label]\n",
    "        sampled_label = label_samples.sample(\n",
    "            n=min(malicious_label_rows, len(label_samples)), \n",
    "            random_state=42\n",
    "        )\n",
    "        sampled_malicious = pd.concat([sampled_malicious, sampled_label])\n",
    "    \n",
    "    # Combine sampled datasets\n",
    "    sampled_df = pd.concat([sampled_benign, sampled_malicious])\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    sampled_df = sampled_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Print new label distribution\n",
    "    print(\"\\nSampled Label Distribution:\")\n",
    "    print(sampled_df['Label'].value_counts())\n",
    "    \n",
    "    return sampled_df\n",
    "\n",
    "dtype_dict = {\n",
    "    'Label': 'category'\n",
    "}\n",
    "\n",
    "# Read the original dataset\n",
    "df = pd.read_csv('/teamspace/studios/this_studio/Anomoly_Threat_Detection_using_GNNs/datasets/cicids2017/all_data.csv', dtype= dtype_dict)\n",
    "\n",
    "# Sample the dataset\n",
    "sampled_df = sample_balanced_dataset(df, total_rows=50000)\n",
    "output_dir = '/teamspace/studios/this_studio/Anomoly_Threat_Detection_using_GNNs/datasets/cicids2017/'\n",
    "\n",
    "# Save sampled dataset\n",
    "sampled_dataset_path = os.path.join(output_dir, 's4_all_data.csv')\n",
    "sampled_df.to_csv(sampled_dataset_path, index=False)\n",
    "print(f\"\\nSampled dataset saved to: {sampled_dataset_path}\")\n",
    "\n",
    "# # Import graph generator (assuming it's in the same directory)\n",
    "# from graph_generator import CICIDSGraphGenerator, split_and_save_graphs\n",
    "\n",
    "# # Generate graphs\n",
    "# # Set binary or multi-class classification\n",
    "# binary_classification = False  # Set to True for binary classification\n",
    "\n",
    "# # Generate and save graphs\n",
    "# graphs = split_and_save_graphs(\n",
    "#     sampled_df, \n",
    "#     save_dir=output_dir, \n",
    "#     binary_classification=binary_classification\n",
    "# )\n",
    "\n",
    "# print(\"\\nGraph generation complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
